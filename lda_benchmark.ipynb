{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with 279 Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "{'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court', 'data_dir': '/usr/local/lib/python3.5/dist-packages/textacy/data/supreme_court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.'}\n",
      "Found 8419 documents.\n",
      "Found 279 labels.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from textacy.datasets.supreme_court import SupremeCourt\n",
    "\n",
    "print('Processing text dataset')\n",
    "\n",
    "sc = SupremeCourt()\n",
    "print(sc.info)\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "issue_codes = list(sc.issue_codes.keys())\n",
    "issue_codes.append('-1')\n",
    "issue_codes.sort()\n",
    "\n",
    "labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "\n",
    "for i,record in enumerate(sc.records(limit=-1)):\n",
    "    if record['issue'] == None: # some cases have None as an issue\n",
    "        labels.append(labels_index['-1'])\n",
    "    else:\n",
    "        labels.append(labels_index[record['issue']])\n",
    "    texts.append(record['text'])\n",
    "\n",
    "print('Found %s documents.' % len(texts))\n",
    "print('Found %s labels.' % len(labels_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "from gensim import corpora, models\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "docs = []\n",
    "\n",
    "for text in texts:\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    stopped_tokens = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stopped_tokens]\n",
    "    docs.append(lemmatized_tokens)\n",
    "    \n",
    "texts = docs\n",
    "    \n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "\n",
    "dictionary.filter_extremes()\n",
    "dictionary.compactify()\n",
    "dictionary.save_as_text('scotus.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load_from_text('scotus.dict')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('scotus_corpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-01 23:38:00,900 : INFO : loaded corpus index from scotus_corpus.mm.index\n",
      "2018-04-01 23:38:00,901 : INFO : initializing cython corpus reader from scotus_corpus.mm\n",
      "2018-04-01 23:38:00,902 : INFO : accepted corpus with 8419 documents, 56365 features, 7829471 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "corpus = corpora.MmCorpus('scotus_corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-01 23:38:00,907 : INFO : collecting document frequencies\n",
      "2018-04-01 23:38:00,908 : INFO : PROGRESS: processing document #0\n",
      "2018-04-01 23:38:06,497 : INFO : calculating IDF weights for 8419 documents and 56364 features (7829471 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "tfidf_model = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tfidf_model.save('scotus_TfidfModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf_model[corpus]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(corpus_tfidf[0]))\n",
    "print(len(corpus_tfidf[1]))\n",
    "corpus_tfidf[0][0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-01 23:38:06,648 : INFO : using symmetric alpha at 0.0033333333333333335\n",
      "2018-04-01 23:38:06,649 : INFO : using symmetric eta at 0.0033333333333333335\n",
      "2018-04-01 23:38:06,660 : INFO : using serial LDA version on this node\n",
      "2018-04-01 23:38:08,938 : INFO : running online (single-pass) LDA training, 300 topics, 1 passes over the supplied corpus of 8419 documents, updating model once every 2000 documents, evaluating perplexity every 8419 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2018-04-01 23:38:08,939 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2018-04-01 23:38:12,053 : INFO : PROGRESS: pass 0, at document #2000/8419\n",
      "2018-04-01 23:38:23,196 : INFO : merging changes from 2000 documents into a model of 8419 documents\n",
      "2018-04-01 23:38:25,452 : INFO : topic #233 (0.003): 0.005*\"enemy\" + 0.004*\"cement\" + 0.004*\"poy\" + 0.004*\"labor\" + 0.003*\"taxicab\" + 0.003*\"chin\" + 0.003*\"alstate\" + 0.003*\"ally\" + 0.003*\"collins\" + 0.003*\"pennsylvania\"\n",
      "2018-04-01 23:38:25,453 : INFO : topic #242 (0.003): 0.007*\"tax\" + 0.004*\"confession\" + 0.004*\"bankruptcy\" + 0.003*\"carry-back\" + 0.003*\"camp\" + 0.003*\"civilian\" + 0.003*\"exemption\" + 0.003*\"testificandum\" + 0.003*\"sain\" + 0.003*\"georgia\"\n",
      "2018-04-01 23:38:25,455 : INFO : topic #218 (0.003): 0.007*\"112\" + 0.006*\"stock\" + 0.005*\"reorganization\" + 0.004*\"greenville\" + 0.004*\"obscene\" + 0.003*\"bookseller\" + 0.003*\"244.\" + 0.003*\"voting\" + 0.003*\"obscenity\" + 0.003*\"exchange\"\n",
      "2018-04-01 23:38:25,456 : INFO : topic #24 (0.003): 0.007*\"yardmaster\" + 0.006*\"gmac\" + 0.004*\"tax\" + 0.004*\"virginia\" + 0.004*\"decree\" + 0.004*\"dealer\" + 0.003*\"water\" + 0.003*\"commission\" + 0.003*\"school\" + 0.003*\"irrigation\"\n",
      "2018-04-01 23:38:25,457 : INFO : topic #6 (0.003): 0.007*\"2590\" + 0.005*\"prosequendum\" + 0.004*\"arbitration\" + 0.004*\"preference\" + 0.004*\"veteran\" + 0.004*\"commission\" + 0.004*\"reemployment\" + 0.003*\"habeas\" + 0.003*\"corpus\" + 0.003*\"tax\"\n",
      "2018-04-01 23:38:25,490 : INFO : topic diff=276.940704, rho=1.000000\n",
      "2018-04-01 23:38:29,146 : INFO : PROGRESS: pass 0, at document #4000/8419\n",
      "/usr/local/lib/python3.5/dist-packages/gensim/models/ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "2018-04-01 23:38:41,781 : INFO : merging changes from 2000 documents into a model of 8419 documents\n",
      "2018-04-01 23:38:44,022 : INFO : topic #3 (0.003): 0.012*\"kinoy\" + 0.007*\"processor\" + 0.006*\"repossession\" + 0.005*\"log\" + 0.005*\"conditional\" + 0.004*\"378\" + 0.003*\"1263\" + 0.003*\"military\" + 0.003*\"schilder\" + 0.003*\"tax\"\n",
      "2018-04-01 23:38:44,023 : INFO : topic #272 (0.003): 0.017*\"holiday\" + 0.008*\"kelly\" + 0.006*\"promise\" + 0.005*\"clancy\" + 0.004*\"diem\" + 0.004*\"carroll\" + 0.003*\"verbatim\" + 0.003*\"artificial\" + 0.003*\"office\" + 0.003*\"witness\"\n",
      "2018-04-01 23:38:44,025 : INFO : topic #169 (0.003): 0.010*\"abstention\" + 0.008*\"citizenship\" + 0.006*\"termination\" + 0.006*\"pollution\" + 0.005*\"contempt\" + 0.004*\"valorem\" + 0.004*\"election\" + 0.004*\"42\" + 0.004*\"mississippi\" + 0.004*\"grand\"\n",
      "2018-04-01 23:38:44,026 : INFO : topic #97 (0.003): 0.013*\"arrest\" + 0.008*\"hill\" + 0.006*\"cheyenne\" + 0.006*\"door\" + 0.005*\"wildlife\" + 0.005*\"apartment\" + 0.005*\"venue\" + 0.005*\"heller\" + 0.004*\"officer\" + 0.004*\"3109\"\n",
      "2018-04-01 23:38:44,028 : INFO : topic #35 (0.003): 0.008*\"retail\" + 0.006*\"sale\" + 0.005*\"00\" + 0.005*\"7237\" + 0.005*\"referee\" + 0.005*\"unemployment\" + 0.004*\"lucas\" + 0.004*\"establishment\" + 0.004*\"gross\" + 0.004*\"sentence\"\n",
      "2018-04-01 23:38:44,061 : INFO : topic diff=inf, rho=0.707107\n",
      "2018-04-01 23:38:49,097 : INFO : PROGRESS: pass 0, at document #6000/8419\n",
      "2018-04-01 23:39:06,183 : INFO : merging changes from 2000 documents into a model of 8419 documents\n",
      "2018-04-01 23:39:08,426 : INFO : topic #239 (0.003): 0.009*\"charter\" + 0.008*\"mayor\" + 0.008*\"armco\" + 0.007*\"apprenticeship\" + 0.006*\"port\" + 0.005*\"nonfrivolous\" + 0.005*\"pane\" + 0.005*\"lieberman\" + 0.004*\"philadelphia\" + 0.004*\"pilot\"\n",
      "2018-04-01 23:39:08,427 : INFO : topic #136 (0.003): 0.010*\"massachusetts\" + 0.009*\"constable\" + 0.007*\"tire\" + 0.005*\"fender\" + 0.005*\"merger\" + 0.005*\"gibbs\" + 0.004*\"cabin\" + 0.004*\"pawnshop\" + 0.004*\"weld\" + 0.003*\"punitive\"\n",
      "2018-04-01 23:39:08,428 : INFO : topic #214 (0.003): 0.049*\"irs\" + 0.016*\"501\" + 0.010*\"waller\" + 0.006*\"recess\" + 0.006*\"fuel\" + 0.005*\"estes\" + 0.005*\"reviewability\" + 0.004*\"1254\" + 0.004*\"gasoline\" + 0.004*\"3401\"\n",
      "2018-04-01 23:39:08,430 : INFO : topic #217 (0.003): 0.012*\"nuisance\" + 0.007*\"billboard\" + 0.006*\"alexandria\" + 0.006*\"jewel\" + 0.005*\"stanton\" + 0.005*\"board\" + 0.004*\"kentucky\" + 0.004*\"reinstatement\" + 0.004*\"labor\" + 0.003*\"ordinance\"\n",
      "2018-04-01 23:39:08,431 : INFO : topic #133 (0.003): 0.008*\"joel\" + 0.007*\"disc\" + 0.007*\"grazing\" + 0.007*\"pickering\" + 0.005*\"enclave\" + 0.005*\"abatement\" + 0.005*\"1208\" + 0.005*\"horse\" + 0.004*\"preston\" + 0.004*\"speeding\"\n",
      "2018-04-01 23:39:08,464 : INFO : topic diff=inf, rho=0.577350\n",
      "2018-04-01 23:39:13,921 : INFO : PROGRESS: pass 0, at document #8000/8419\n",
      "2018-04-01 23:39:32,274 : INFO : merging changes from 2000 documents into a model of 8419 documents\n",
      "2018-04-01 23:39:34,517 : INFO : topic #9 (0.003): 0.006*\"truth-seeking\" + 0.002*\"fund-raising\" + 0.002*\"1471\" + 0.002*\"rosen\" + 0.002*\"luttrell\" + 0.001*\"nonlegislative\" + 0.001*\"797.\" + 0.001*\"1473.\" + 0.001*\"administered\" + 0.001*\"oath\"\n",
      "2018-04-01 23:39:34,519 : INFO : topic #174 (0.003): 0.031*\"merger\" + 0.026*\"fdic\" + 0.016*\"bank\" + 0.016*\"ftc\" + 0.011*\"banking\" + 0.007*\"procompetitive\" + 0.005*\"branching\" + 0.005*\"anticompetitive\" + 0.003*\"monterey\" + 0.003*\"acquisition\"\n",
      "2018-04-01 23:39:34,520 : INFO : topic #62 (0.003): 0.039*\"eleventh\" + 0.025*\"compact\" + 0.019*\"state-law\" + 0.011*\"1871\" + 0.011*\"1979\" + 0.010*\"female\" + 0.010*\"globe\" + 0.009*\"42d\" + 0.008*\"starr\" + 0.006*\"chisholm\"\n",
      "2018-04-01 23:39:34,522 : INFO : topic #31 (0.003): 0.018*\"union\" + 0.015*\"plea\" + 0.012*\"eleventh\" + 0.011*\"coal\" + 0.010*\"representation\" + 0.009*\"labor\" + 0.009*\"employer\" + 0.007*\"waiver\" + 0.006*\"guilty\" + 0.006*\"employee\"\n",
      "2018-04-01 23:39:34,523 : INFO : topic #82 (0.003): 0.011*\"q.\" + 0.010*\"muniz\" + 0.008*\"n't\" + 0.007*\"88th\" + 0.005*\"hypnosis\" + 0.005*\"yeah\" + 0.005*\"warning\" + 0.004*\"supersedeas\" + 0.004*\"'ll\" + 0.004*\"'m\"\n",
      "2018-04-01 23:39:34,555 : INFO : topic diff=inf, rho=0.500000\n",
      "2018-04-01 23:39:50,331 : INFO : -70.945 per-word bound, 2272155088403132317696.0 perplexity estimate based on a held-out corpus of 419 documents with 4866 words\n",
      "2018-04-01 23:39:50,331 : INFO : PROGRESS: pass 0, at document #8419/8419\n",
      "2018-04-01 23:39:54,357 : INFO : merging changes from 419 documents into a model of 8419 documents\n",
      "2018-04-01 23:39:56,599 : INFO : topic #218 (0.003): 0.014*\"scienter\" + 0.004*\"purification\" + 0.003*\"court-approved\" + 0.003*\"liv\" + 0.002*\"voting\" + 0.002*\"bookseller\" + 0.002*\"tallying\" + 0.002*\"nabrit\" + 0.001*\"durham\" + 0.001*\"712-713.\"\n",
      "2018-04-01 23:39:56,600 : INFO : topic #36 (0.003): 0.011*\"caldwell\" + 0.009*\"hillman\" + 0.008*\"spending\" + 0.008*\"three-judge\" + 0.007*\"case-or-controversy\" + 0.006*\"subrogation\" + 0.006*\"1291\" + 0.006*\"belton\" + 0.005*\"frothingham\" + 0.005*\"appealable\"\n",
      "2018-04-01 23:39:56,601 : INFO : topic #15 (0.003): 0.011*\"sawed-off\" + 0.008*\"southerly\" + 0.008*\"eldridge\" + 0.006*\"comptroller\" + 0.005*\"affiant\" + 0.004*\"borrowing\" + 0.004*\"personal-injury\" + 0.003*\"'cause\" + 0.003*\"34a\" + 0.003*\"stoddard\"\n",
      "2018-04-01 23:39:56,603 : INFO : topic #168 (0.003): 0.028*\"chevron\" + 0.020*\"1981\" + 0.016*\"phone\" + 0.015*\"deference\" + 0.014*\"objectively\" + 0.010*\"pharmaceutical\" + 0.009*\"cap\" + 0.007*\"robbins\" + 0.007*\"retarded\" + 0.006*\"bag\"\n",
      "2018-04-01 23:39:56,604 : INFO : topic #147 (0.003): 0.017*\"nlra\" + 0.014*\"union\" + 0.014*\"employer\" + 0.014*\"sign\" + 0.013*\"nlrb\" + 0.011*\"bargaining\" + 0.011*\"employee\" + 0.009*\"ha\" + 0.009*\"labor\" + 0.008*\"collective-bargaining\"\n",
      "2018-04-01 23:39:56,636 : INFO : topic diff=inf, rho=0.447214\n"
     ]
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lda = lda[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-01 23:39:56,667 : INFO : saving LdaState object under scotus.lda.state, separately None\n",
      "2018-04-01 23:39:56,668 : INFO : storing np array 'sstats' to scotus.lda.state.sstats.npy\n",
      "2018-04-01 23:39:56,714 : INFO : saved scotus.lda.state\n",
      "2018-04-01 23:39:56,738 : INFO : saving LdaModel object under scotus.lda, separately ['expElogbeta', 'sstats']\n",
      "2018-04-01 23:39:56,738 : INFO : not storing attribute state\n",
      "2018-04-01 23:39:56,739 : INFO : not storing attribute dispatcher\n",
      "2018-04-01 23:39:56,739 : INFO : storing np array 'expElogbeta' to scotus.lda.expElogbeta.npy\n",
      "2018-04-01 23:39:56,874 : INFO : not storing attribute id2word\n",
      "2018-04-01 23:39:56,876 : INFO : saved scotus.lda\n"
     ]
    }
   ],
   "source": [
    "lda.save('scotus.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data for classification\n",
    "import gensim\n",
    "X = np.transpose(gensim.matutils.corpus2dense(corpus_lda, num_terms=90018))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6735, 90018)\n",
      "(6735,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1684, 90018)\n",
      "(1684,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1342042755344418\n"
     ]
    }
   ],
   "source": [
    "score = logreg.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classification with 15 Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court', 'data_dir': '/usr/local/lib/python3.5/dist-packages/textacy/data/supreme_court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.'}\n",
      "Found 8419 texts.\n",
      "Found 15 labels.\n"
     ]
    }
   ],
   "source": [
    "sc = SupremeCourt()\n",
    "print(sc.info)\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "issue_codes = list(sc.issue_area_codes.keys())\n",
    "issue_codes.sort()\n",
    "issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "\n",
    "for i,record in enumerate(sc.records(limit=-1)):\n",
    "    if record['issue'] == None: # some cases have None as an issue\n",
    "        labels.append(labels_index['-1'])\n",
    "    else:\n",
    "        labels.append(labels_index[record['issue'][:-4]])\n",
    "    texts.append(record['text'])\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "print('Found %s labels.' % len(labels_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6735, 90018)\n",
      "(6735,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1684, 90018)\n",
      "(1684,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40320665083135393\n"
     ]
    }
   ],
   "source": [
    "score = logreg.score(X_test, y_test)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
