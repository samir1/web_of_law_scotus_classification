{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with 279 Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "{'data_dir': '/usr/local/lib/python3.5/dist-packages/textacy/data/supreme_court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.', 'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court'}\n",
      "Found 8419 documents.\n",
      "Found 279 labels.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from textacy.datasets.supreme_court import SupremeCourt\n",
    "\n",
    "print('Processing text dataset')\n",
    "\n",
    "sc = SupremeCourt()\n",
    "print(sc.info)\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "issue_codes = list(sc.issue_codes.keys())\n",
    "issue_codes.append('-1')\n",
    "issue_codes.sort()\n",
    "\n",
    "labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "\n",
    "for i,record in enumerate(sc.records(limit=-1)):\n",
    "    if record['issue'] == None: # some cases have None as an issue\n",
    "        labels.append(labels_index['-1'])\n",
    "    else:\n",
    "        labels.append(labels_index[record['issue']])\n",
    "    texts.append(record['text'])\n",
    "\n",
    "print('Found %s documents.' % len(texts))\n",
    "print('Found %s labels.' % len(labels_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "from gensim import corpora, models\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "docs = []\n",
    "\n",
    "for text in texts:\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    stopped_tokens = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stopped_tokens]\n",
    "    docs.append(lemmatized_tokens)\n",
    "    \n",
    "texts = docs\n",
    "    \n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "\n",
    "dictionary.filter_extremes()\n",
    "dictionary.compactify()\n",
    "dictionary.save_as_text('scotus.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load_from_text('scotus.dict')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('scotus_corpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-10 02:50:20,450 : INFO : loaded corpus index from scotus_corpus.mm.index\n",
      "2018-07-10 02:50:20,451 : INFO : initializing cython corpus reader from scotus_corpus.mm\n",
      "2018-07-10 02:50:20,452 : INFO : accepted corpus with 8419 documents, 56365 features, 7829471 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "corpus = corpora.MmCorpus('scotus_corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-10 02:50:20,457 : INFO : collecting document frequencies\n",
      "2018-07-10 02:50:20,458 : INFO : PROGRESS: processing document #0\n",
      "2018-07-10 02:50:26,676 : INFO : calculating IDF weights for 8419 documents and 56364 features (7829471 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "tfidf_model = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tfidf_model.save('scotus_TfidfModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-10 02:50:26,828 : INFO : using symmetric alpha at 0.0033333333333333335\n",
      "2018-07-10 02:50:26,829 : INFO : using symmetric eta at 0.0033333333333333335\n",
      "2018-07-10 02:50:26,841 : INFO : using serial LDA version on this node\n",
      "2018-07-10 02:50:29,186 : INFO : running online (single-pass) LDA training, 300 topics, 1 passes over the supplied corpus of 8419 documents, updating model once every 2000 documents, evaluating perplexity every 8419 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2018-07-10 02:50:29,187 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2018-07-10 02:50:32,520 : INFO : PROGRESS: pass 0, at document #2000/8419\n",
      "2018-07-10 02:51:29,176 : INFO : merging changes from 2000 documents into a model of 8419 documents\n",
      "2018-07-10 02:51:32,613 : INFO : topic #112 (0.003): 0.006*\"tax\" + 0.003*\"leasehold\" + 0.003*\"commerce\" + 0.003*\"interstate\" + 0.003*\"kaiser\" + 0.003*\"rent\" + 0.002*\"bulova\" + 0.002*\"indictment\" + 0.002*\"arbitration\" + 0.002*\"landlord\"\n",
      "2018-07-10 02:51:32,615 : INFO : topic #233 (0.003): 0.005*\"sentence\" + 0.004*\"offering\" + 0.004*\"erbe\" + 0.004*\"count\" + 0.004*\"ribbon\" + 0.004*\"improvidently\" + 0.004*\"employee\" + 0.003*\"jury\" + 0.003*\"flaxer\" + 0.003*\"issuer\"\n",
      "2018-07-10 02:51:32,618 : INFO : topic #79 (0.003): 0.010*\"partnership\" + 0.007*\"easement\" + 0.005*\"culbertson\" + 0.004*\"tower\" + 0.003*\"tax\" + 0.003*\"income\" + 0.003*\"fleischman\" + 0.003*\"partner\" + 0.003*\"gain\" + 0.003*\"capital\"\n",
      "2018-07-10 02:51:32,621 : INFO : topic #191 (0.003): 0.005*\"union\" + 0.004*\"communist\" + 0.004*\"affidavit\" + 0.004*\"divorce\" + 0.004*\"virgin\" + 0.004*\"island\" + 0.004*\"naturalization\" + 0.003*\"shop\" + 0.003*\"labor\" + 0.003*\"board\"\n",
      "2018-07-10 02:51:32,623 : INFO : topic #166 (0.003): 0.008*\"compact\" + 0.005*\"patent\" + 0.005*\"metallic\" + 0.005*\"overtime\" + 0.005*\"hour\" + 0.004*\"liquor\" + 0.003*\"licensor\" + 0.003*\"michigan\" + 0.003*\"royalty\" + 0.003*\"rate\"\n",
      "2018-07-10 02:51:32,670 : INFO : topic diff=277.086334, rho=1.000000\n",
      "2018-07-10 02:51:40,139 : INFO : PROGRESS: pass 0, at document #4000/8419\n",
      "/usr/local/lib/python3.5/dist-packages/gensim/models/ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "2018-07-10 02:54:01,431 : INFO : merging changes from 2000 documents into a model of 8419 documents\n",
      "2018-07-10 02:54:04,874 : INFO : topic #246 (0.003): 0.020*\"indian\" + 0.010*\"tribe\" + 0.007*\"hunting\" + 0.006*\"dickey\" + 0.005*\"damage\" + 0.005*\"morrissey\" + 0.005*\"acme\" + 0.005*\"fault\" + 0.005*\"peninsula\" + 0.004*\"bid\"\n",
      "2018-07-10 02:54:04,879 : INFO : topic #14 (0.003): 0.008*\"mcfadden\" + 0.007*\"injunctive\" + 0.007*\"davis\" + 0.006*\"privilege\" + 0.005*\"self-incrimination\" + 0.005*\"1441\" + 0.005*\"removal\" + 0.005*\"superior\" + 0.004*\"raleigh\" + 0.004*\"carolina\"\n",
      "2018-07-10 02:54:04,884 : INFO : topic #247 (0.003): 0.007*\"enrollment\" + 0.006*\"penfield\" + 0.006*\"fine\" + 0.005*\"deviation\" + 0.005*\"tax\" + 0.005*\"coercive\" + 0.004*\"young\" + 0.004*\"photograph\" + 0.004*\"contempt\" + 0.004*\"bankruptcy\"\n",
      "2018-07-10 02:54:04,889 : INFO : topic #251 (0.003): 0.006*\"israel\" + 0.006*\"annuity\" + 0.006*\"sabine\" + 0.006*\"investment\" + 0.005*\"investor\" + 0.004*\"kopp\" + 0.004*\"2282\" + 0.004*\"chester\" + 0.004*\"grunewald\" + 0.004*\"373\"\n",
      "2018-07-10 02:54:04,894 : INFO : topic #101 (0.003): 0.012*\"depreciation\" + 0.004*\"curiosity\" + 0.004*\"commingled\" + 0.003*\"tax\" + 0.003*\"straight-line\" + 0.003*\"ivan\" + 0.003*\"magnano\" + 0.003*\"1.167\" + 0.002*\"167\" + 0.002*\"trade-in\"\n",
      "2018-07-10 02:54:04,939 : INFO : topic diff=inf, rho=0.707107\n",
      "2018-07-10 02:54:14,831 : INFO : PROGRESS: pass 0, at document #6000/8419\n",
      "2018-07-10 02:57:03,201 : INFO : merging changes from 2000 documents into a model of 8419 documents\n",
      "2018-07-10 02:57:06,597 : INFO : topic #63 (0.003): 0.009*\"o'shea\" + 0.006*\"incremental\" + 0.005*\"nacional\" + 0.005*\"owner-operators\" + 0.004*\"dutcher\" + 0.004*\"foreign-flag\" + 0.004*\"prairie\" + 0.004*\"falstaff\" + 0.004*\"incres\" + 0.003*\"tractor\"\n",
      "2018-07-10 02:57:06,599 : INFO : topic #280 (0.003): 0.011*\"dea\" + 0.010*\"electricity\" + 0.009*\"marc\" + 0.009*\"noncommercial\" + 0.007*\"sport\" + 0.007*\"gerald\" + 0.005*\"meier\" + 0.005*\"renegotiation\" + 0.004*\"lockheed\" + 0.004*\"racing\"\n",
      "2018-07-10 02:57:06,607 : INFO : topic #29 (0.003): 0.015*\"gagnon\" + 0.006*\"theatrical\" + 0.005*\"editorializing\" + 0.004*\"transit\" + 0.004*\"stockyard\" + 0.004*\"kenosha\" + 0.003*\"booking\" + 0.003*\"recuse\" + 0.003*\"certificate\" + 0.003*\"inadvertence\"\n",
      "2018-07-10 02:57:06,610 : INFO : topic #5 (0.003): 0.044*\"nlra\" + 0.019*\"lmra\" + 0.018*\"ila\" + 0.013*\"stevedore\" + 0.010*\"shipowner\" + 0.009*\"stevedoring\" + 0.008*\"longshoreman\" + 0.006*\"ship\" + 0.006*\"perko\" + 0.006*\"anker\"\n",
      "2018-07-10 02:57:06,612 : INFO : topic #243 (0.003): 0.009*\"goldman\" + 0.004*\"sunnen\" + 0.004*\"attainder\" + 0.003*\"seymour\" + 0.003*\"erdahl\" + 0.002*\"garland\" + 0.002*\"bishop\" + 0.002*\"charter\" + 0.001*\"252\" + 0.001*\"goetz\"\n",
      "2018-07-10 02:57:06,658 : INFO : topic diff=inf, rho=0.577350\n",
      "2018-07-10 02:57:17,801 : INFO : PROGRESS: pass 0, at document #8000/8419\n",
      "2018-07-10 03:00:08,069 : INFO : merging changes from 2000 documents into a model of 8419 documents\n",
      "2018-07-10 03:00:11,499 : INFO : topic #34 (0.003): 0.012*\"informant\" + 0.011*\"esa\" + 0.009*\"706\" + 0.009*\"aguilar\" + 0.006*\"ginsburg\" + 0.006*\"shepard\" + 0.006*\"eric\" + 0.005*\"apartment\" + 0.005*\"jones\" + 0.005*\"search\"\n",
      "2018-07-10 03:00:11,501 : INFO : topic #157 (0.003): 0.017*\"tariff\" + 0.004*\"14-day\" + 0.002*\"hollander\" + 0.001*\"rader\" + 0.001*\"medalie\" + 0.001*\"morton\" + 0.001*\"pioneer\" + 0.001*\"lumber\" + 0.001*\"diversion\" + 0.001*\"railroad\"\n",
      "2018-07-10 03:00:11,503 : INFO : topic #217 (0.003): 0.020*\"second-degree\" + 0.013*\"2518\" + 0.011*\"co-op\" + 0.009*\"wyoming\" + 0.008*\"intervene\" + 0.006*\"tenure\" + 0.006*\"migrant\" + 0.005*\"intervention\" + 0.005*\"eyewitness\" + 0.004*\"judgeship\"\n",
      "2018-07-10 03:00:11,505 : INFO : topic #135 (0.003): 0.031*\"prejudgment\" + 0.021*\"heck\" + 0.014*\"darden\" + 0.008*\"quota\" + 0.006*\"cotton\" + 0.006*\"suitcase\" + 0.005*\"m/v\" + 0.004*\"1295\" + 0.003*\"1330\" + 0.003*\"2106\"\n",
      "2018-07-10 03:00:11,507 : INFO : topic #101 (0.003): 0.003*\"depreciation\" + 0.002*\"ivan\" + 0.002*\"pfeifer\" + 0.002*\"1.167\" + 0.002*\"nonexcludable\" + 0.002*\"curiosity\" + 0.001*\"straight-line\" + 0.001*\"magnano\" + 0.001*\"worksheet\" + 0.001*\"commingled\"\n",
      "2018-07-10 03:00:11,549 : INFO : topic diff=inf, rho=0.500000\n",
      "2018-07-10 03:01:04,734 : INFO : -70.526 per-word bound, 1700325126481857740800.0 perplexity estimate based on a held-out corpus of 419 documents with 4866 words\n",
      "2018-07-10 03:01:04,735 : INFO : PROGRESS: pass 0, at document #8419/8419\n",
      "2018-07-10 03:01:40,122 : INFO : merging changes from 419 documents into a model of 8419 documents\n",
      "2018-07-10 03:01:43,347 : INFO : topic #283 (0.003): 0.052*\"search\" + 0.020*\"forfeiture\" + 0.017*\"contraband\" + 0.015*\"arrest\" + 0.015*\"seizure\" + 0.014*\"vehicle\" + 0.012*\"barker\" + 0.012*\"forfeitable\" + 0.011*\"searched\" + 0.011*\"automobile\"\n",
      "2018-07-10 03:01:43,353 : INFO : topic #66 (0.003): 0.027*\"issuer\" + 0.018*\"insider\" + 0.012*\"abc\" + 0.011*\"investment\" + 0.007*\"two-step\" + 0.007*\"income\" + 0.006*\"profit\" + 0.005*\"stockholder\" + 0.004*\"security\" + 0.004*\"rate\"\n",
      "2018-07-10 03:01:43,359 : INFO : topic #242 (0.003): 0.003*\"daley\" + 0.001*\"complemented\" + 0.001*\"bettor\" + 0.001*\"wager\" + 0.001*\"pick-up\" + 0.001*\"bet\" + 0.000*\"3290\" + 0.000*\"`numbers\" + 0.000*\"wagering\" + 0.000*\"excise\"\n",
      "2018-07-10 03:01:43,362 : INFO : topic #71 (0.003): 0.013*\"pendent\" + 0.012*\"vindictiveness\" + 0.007*\"lthough\" + 0.005*\"nonfrivolous\" + 0.003*\"felton\" + 0.003*\"schwab\" + 0.003*\"gideon\" + 0.003*\"earle\" + 0.003*\"byers\" + 0.002*\"wainwright\"\n",
      "2018-07-10 03:01:43,364 : INFO : topic #214 (0.003): 0.015*\"seibert\" + 0.014*\"flyer\" + 0.010*\"church\" + 0.008*\"congregation\" + 0.007*\"sheppard\" + 0.007*\"synod\" + 0.004*\"archbishop\" + 0.004*\"ecclesiastical\" + 0.003*\"4003\" + 0.003*\"diocese\"\n",
      "2018-07-10 03:01:43,410 : INFO : topic diff=inf, rho=0.447214\n"
     ]
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lda = lda[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-10 03:01:43,448 : INFO : saving LdaState object under scotus.lda.state, separately None\n",
      "2018-07-10 03:01:43,449 : INFO : storing np array 'sstats' to scotus.lda.state.sstats.npy\n",
      "2018-07-10 03:01:43,623 : INFO : saved scotus.lda.state\n",
      "2018-07-10 03:01:43,684 : INFO : saving LdaModel object under scotus.lda, separately ['expElogbeta', 'sstats']\n",
      "2018-07-10 03:01:43,689 : INFO : not storing attribute state\n",
      "2018-07-10 03:01:43,693 : INFO : not storing attribute dispatcher\n",
      "2018-07-10 03:01:43,695 : INFO : not storing attribute id2word\n",
      "2018-07-10 03:01:43,695 : INFO : storing np array 'expElogbeta' to scotus.lda.expElogbeta.npy\n",
      "2018-07-10 03:01:43,868 : INFO : saved scotus.lda\n"
     ]
    }
   ],
   "source": [
    "lda.save('scotus.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data for classification\n",
    "import gensim\n",
    "X = np.transpose(gensim.matutils.corpus2dense(corpus_lda, num_terms=90018))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6819, 90018)\n",
      "(6819,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(842, 90018)\n",
      "(842,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10451306413301663\n",
      "0.12269129287598944\n"
     ]
    }
   ],
   "source": [
    "score = logreg.score(X_test, y_test)\n",
    "print(score)\n",
    "score = logreg.score(X_val, y_val)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.00      0.00      0.00         0\n",
      "          2       0.00      0.00      0.00         8\n",
      "          3       0.00      0.00      0.00        11\n",
      "          4       0.00      0.00      0.00         1\n",
      "          5       0.00      0.00      0.00         2\n",
      "          6       0.00      0.00      0.00         2\n",
      "          7       0.00      0.00      0.00         1\n",
      "          8       0.00      0.00      0.00         0\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.00      0.00      0.00         4\n",
      "         11       0.00      0.00      0.00         2\n",
      "         12       0.00      0.00      0.00         1\n",
      "         13       0.00      0.00      0.00         3\n",
      "         14       0.00      0.00      0.00         2\n",
      "         15       0.15      0.75      0.25        24\n",
      "         16       0.00      0.00      0.00         4\n",
      "         17       0.00      0.00      0.00         2\n",
      "         18       0.06      0.94      0.11        17\n",
      "         19       0.00      0.00      0.00        11\n",
      "         20       0.00      0.00      0.00         0\n",
      "         21       0.00      0.00      0.00         6\n",
      "         22       0.00      0.00      0.00         8\n",
      "         23       0.00      0.00      0.00         4\n",
      "         24       0.00      0.00      0.00         2\n",
      "         25       0.00      0.00      0.00         6\n",
      "         26       0.00      0.00      0.00        12\n",
      "         27       0.00      0.00      0.00         2\n",
      "         28       0.00      0.00      0.00         2\n",
      "         29       0.00      0.00      0.00         1\n",
      "         30       0.27      0.64      0.38        11\n",
      "         31       0.00      0.00      0.00         1\n",
      "         32       0.00      0.00      0.00         2\n",
      "         33       0.00      0.00      0.00         1\n",
      "         34       0.00      0.00      0.00         2\n",
      "         35       0.00      0.00      0.00         3\n",
      "         36       0.00      0.00      0.00         2\n",
      "         37       0.00      0.00      0.00         0\n",
      "         38       0.00      0.00      0.00         2\n",
      "         39       0.00      0.00      0.00         1\n",
      "         40       0.00      0.00      0.00         6\n",
      "         41       0.00      0.00      0.00         1\n",
      "         42       0.00      0.00      0.00         0\n",
      "         43       0.00      0.00      0.00         1\n",
      "         44       0.00      0.00      0.00         0\n",
      "         45       0.00      0.00      0.00         0\n",
      "         46       0.00      0.00      0.00         2\n",
      "         47       0.00      0.00      0.00         1\n",
      "         48       0.00      0.00      0.00         3\n",
      "         49       0.00      0.00      0.00         0\n",
      "         50       0.00      0.00      0.00         7\n",
      "         51       0.00      0.00      0.00         0\n",
      "         52       0.00      0.00      0.00         0\n",
      "         53       0.00      0.00      0.00         1\n",
      "         54       0.00      0.00      0.00         0\n",
      "         55       0.00      0.00      0.00         1\n",
      "         56       0.00      0.00      0.00         3\n",
      "         57       0.00      0.00      0.00         3\n",
      "         58       0.00      0.00      0.00         1\n",
      "         59       0.00      0.00      0.00         0\n",
      "         60       0.00      0.00      0.00         0\n",
      "         61       0.00      0.00      0.00         0\n",
      "         62       0.00      0.00      0.00         7\n",
      "         63       0.00      0.00      0.00         0\n",
      "         64       0.00      0.00      0.00         0\n",
      "         65       0.00      0.00      0.00         0\n",
      "         66       0.00      0.00      0.00         0\n",
      "         67       0.00      0.00      0.00         0\n",
      "         68       0.00      0.00      0.00         0\n",
      "         69       0.00      0.00      0.00         7\n",
      "         70       0.00      0.00      0.00         4\n",
      "         71       0.00      0.00      0.00         4\n",
      "         72       0.00      0.00      0.00         1\n",
      "         73       0.00      0.00      0.00         0\n",
      "         74       0.00      0.00      0.00         3\n",
      "         75       0.00      0.00      0.00         1\n",
      "         76       0.00      0.00      0.00         1\n",
      "         77       0.00      0.00      0.00         0\n",
      "         78       0.11      0.80      0.20        20\n",
      "         79       0.00      0.00      0.00         4\n",
      "         80       0.00      0.00      0.00         1\n",
      "         81       0.00      0.00      0.00         0\n",
      "         82       0.00      0.00      0.00         0\n",
      "         83       0.00      0.00      0.00         1\n",
      "         84       0.00      0.00      0.00         0\n",
      "         85       0.00      0.00      0.00         0\n",
      "         86       0.00      0.00      0.00         0\n",
      "         87       0.00      0.00      0.00         0\n",
      "         88       0.00      0.00      0.00         0\n",
      "         89       0.00      0.00      0.00         0\n",
      "         90       0.00      0.00      0.00         0\n",
      "         91       0.00      0.00      0.00         0\n",
      "         92       0.00      0.00      0.00         0\n",
      "         93       0.00      0.00      0.00         2\n",
      "         94       0.00      0.00      0.00         3\n",
      "         95       0.00      0.00      0.00         2\n",
      "         96       0.00      0.00      0.00         5\n",
      "         97       0.00      0.00      0.00        11\n",
      "         98       0.00      0.00      0.00         4\n",
      "         99       0.00      0.00      0.00         4\n",
      "        100       0.00      0.00      0.00         0\n",
      "        101       0.00      0.00      0.00         2\n",
      "        102       0.56      0.62      0.59         8\n",
      "        103       0.00      0.00      0.00         1\n",
      "        104       0.00      0.00      0.00        10\n",
      "        105       0.00      0.00      0.00         2\n",
      "        106       0.00      0.00      0.00         2\n",
      "        107       0.00      0.00      0.00         5\n",
      "        108       0.00      0.00      0.00         4\n",
      "        109       1.00      0.50      0.67         6\n",
      "        110       0.00      0.00      0.00         0\n",
      "        111       0.00      0.00      0.00         1\n",
      "        112       0.00      0.00      0.00         4\n",
      "        113       0.00      0.00      0.00         1\n",
      "        114       0.00      0.00      0.00         4\n",
      "        115       0.00      0.00      0.00         0\n",
      "        116       0.00      0.00      0.00         2\n",
      "        117       0.00      0.00      0.00         5\n",
      "        118       0.00      0.00      0.00         4\n",
      "        119       0.00      0.00      0.00         1\n",
      "        120       0.00      0.00      0.00         1\n",
      "        121       0.00      0.00      0.00         1\n",
      "        122       0.00      0.00      0.00         0\n",
      "        123       0.00      0.00      0.00         0\n",
      "        124       0.00      0.00      0.00         1\n",
      "        125       0.00      0.00      0.00         1\n",
      "        126       0.00      0.00      0.00         2\n",
      "        127       0.00      0.00      0.00         0\n",
      "        128       0.00      0.00      0.00         3\n",
      "        129       0.00      0.00      0.00         3\n",
      "        130       0.00      0.00      0.00         3\n",
      "        131       0.00      0.00      0.00         0\n",
      "        132       0.00      0.00      0.00         1\n",
      "        133       0.00      0.00      0.00        12\n",
      "        134       0.00      0.00      0.00         0\n",
      "        135       0.04      0.15      0.07        13\n",
      "        136       0.00      0.00      0.00         0\n",
      "        137       0.00      0.00      0.00         5\n",
      "        138       0.00      0.00      0.00         1\n",
      "        139       0.00      0.00      0.00         2\n",
      "        140       0.00      0.00      0.00         3\n",
      "        141       0.00      0.00      0.00         1\n",
      "        142       0.00      0.00      0.00         0\n",
      "        143       0.00      0.00      0.00         0\n",
      "        144       0.00      0.00      0.00         0\n",
      "        145       0.00      0.00      0.00         0\n",
      "        146       0.00      0.00      0.00         2\n",
      "        147       0.00      0.00      0.00         1\n",
      "        148       0.00      0.00      0.00         0\n",
      "        149       0.00      0.00      0.00         6\n",
      "        150       0.00      0.00      0.00         6\n",
      "        151       0.00      0.00      0.00         2\n",
      "        152       0.00      0.00      0.00         5\n",
      "        153       0.00      0.00      0.00         9\n",
      "        154       0.00      0.00      0.00         0\n",
      "        155       0.00      0.00      0.00         7\n",
      "        156       0.00      0.00      0.00         4\n",
      "        157       0.00      0.00      0.00         3\n",
      "        158       0.00      0.00      0.00         4\n",
      "        159       0.00      0.00      0.00         2\n",
      "        160       0.00      0.00      0.00         2\n",
      "        161       0.00      0.00      0.00        11\n",
      "        162       0.00      0.00      0.00         2\n",
      "        163       0.00      0.00      0.00         6\n",
      "        164       0.00      0.00      0.00         0\n",
      "        165       0.00      0.00      0.00         1\n",
      "        166       0.00      0.00      0.00        11\n",
      "        167       0.00      0.00      0.00         0\n",
      "        168       0.00      0.00      0.00         0\n",
      "        169       0.00      0.00      0.00         0\n",
      "        170       0.00      0.00      0.00         5\n",
      "        171       0.00      0.00      0.00         1\n",
      "        172       0.00      0.00      0.00         1\n",
      "        173       0.00      0.00      0.00         3\n",
      "        174       0.00      0.00      0.00         0\n",
      "        175       0.00      0.00      0.00         5\n",
      "        176       0.00      0.00      0.00         2\n",
      "        177       0.00      0.00      0.00         1\n",
      "        178       0.00      0.00      0.00         1\n",
      "        179       0.00      0.00      0.00         3\n",
      "        180       0.00      0.00      0.00         1\n",
      "        181       0.00      0.00      0.00         0\n",
      "        182       0.00      0.00      0.00         2\n",
      "        183       0.00      0.00      0.00         2\n",
      "        184       0.00      0.00      0.00         1\n",
      "        185       0.00      0.00      0.00         2\n",
      "        186       0.00      0.00      0.00         0\n",
      "        187       0.00      0.00      0.00         0\n",
      "        188       0.00      0.00      0.00         0\n",
      "        189       0.00      0.00      0.00         3\n",
      "        190       0.00      0.00      0.00         0\n",
      "        191       0.15      0.74      0.24        19\n",
      "        192       0.00      0.00      0.00         4\n",
      "        193       0.00      0.00      0.00        10\n",
      "        194       0.00      0.00      0.00         7\n",
      "        195       1.00      0.12      0.22         8\n",
      "        196       0.00      0.00      0.00        18\n",
      "        197       0.33      0.15      0.21        13\n",
      "        198       0.00      0.00      0.00         1\n",
      "        199       0.00      0.00      0.00         3\n",
      "        200       0.19      0.27      0.22        11\n",
      "        201       0.00      0.00      0.00         0\n",
      "        202       0.00      0.00      0.00        10\n",
      "        203       0.00      0.00      0.00         6\n",
      "        204       0.00      0.00      0.00         3\n",
      "        205       0.00      0.00      0.00         2\n",
      "        206       0.00      0.00      0.00         0\n",
      "        207       0.00      0.00      0.00         1\n",
      "        208       0.00      0.00      0.00         5\n",
      "        209       0.00      0.00      0.00         2\n",
      "        210       0.00      0.00      0.00         1\n",
      "        211       0.00      0.00      0.00         4\n",
      "        212       0.00      0.00      0.00         1\n",
      "        213       0.00      0.00      0.00         7\n",
      "        214       0.00      0.00      0.00         1\n",
      "        215       0.00      0.00      0.00         4\n",
      "        216       0.00      0.00      0.00         0\n",
      "        217       0.00      0.00      0.00         0\n",
      "        218       0.00      0.00      0.00         1\n",
      "        219       0.00      0.00      0.00         0\n",
      "        220       0.00      0.00      0.00         0\n",
      "        221       0.00      0.00      0.00         2\n",
      "        222       0.00      0.00      0.00         0\n",
      "        223       0.00      0.00      0.00         0\n",
      "        224       0.00      0.00      0.00         0\n",
      "        225       0.00      0.00      0.00         0\n",
      "        226       0.00      0.00      0.00         1\n",
      "        227       0.00      0.00      0.00         0\n",
      "        228       0.00      0.00      0.00         1\n",
      "        229       0.00      0.00      0.00         1\n",
      "        230       0.00      0.00      0.00         1\n",
      "        231       0.00      0.00      0.00         0\n",
      "        232       0.00      0.00      0.00         0\n",
      "        233       0.00      0.00      0.00         0\n",
      "        234       0.00      0.00      0.00         0\n",
      "        235       0.00      0.00      0.00         2\n",
      "        236       0.00      0.00      0.00         0\n",
      "        237       0.00      0.00      0.00        10\n",
      "        238       0.00      0.00      0.00        17\n",
      "        239       0.00      0.00      0.00         5\n",
      "        240       0.00      0.00      0.00         2\n",
      "        241       0.75      0.40      0.52        15\n",
      "        242       0.00      0.00      0.00         1\n",
      "        243       0.00      0.00      0.00         0\n",
      "        244       0.00      0.00      0.00         3\n",
      "        245       0.00      0.00      0.00         1\n",
      "        246       0.00      0.00      0.00         0\n",
      "        247       0.00      0.00      0.00         1\n",
      "        248       0.00      0.00      0.00         0\n",
      "        249       0.00      0.00      0.00         0\n",
      "        250       0.00      0.00      0.00         2\n",
      "        251       0.00      0.00      0.00         2\n",
      "        252       0.00      0.00      0.00         0\n",
      "        253       0.00      0.00      0.00         0\n",
      "        254       0.00      0.00      0.00         3\n",
      "        255       0.00      0.00      0.00         0\n",
      "        256       0.00      0.00      0.00         1\n",
      "        257       0.00      0.00      0.00         0\n",
      "        258       0.00      0.00      0.00         5\n",
      "        259       0.00      0.00      0.00         3\n",
      "        260       0.00      0.00      0.00         2\n",
      "        261       0.00      0.00      0.00         0\n",
      "        262       0.00      0.00      0.00         2\n",
      "        263       0.00      0.00      0.00         2\n",
      "        264       0.00      0.00      0.00         7\n",
      "        265       0.00      0.00      0.00         3\n",
      "        266       0.00      0.00      0.00         0\n",
      "        267       0.00      0.00      0.00         0\n",
      "        268       0.00      0.00      0.00         2\n",
      "        269       0.00      0.00      0.00         0\n",
      "        270       0.00      0.00      0.00         0\n",
      "        271       0.00      0.00      0.00         0\n",
      "        272       0.00      0.00      0.00         1\n",
      "        273       0.00      0.00      0.00         1\n",
      "        274       0.00      0.00      0.00         1\n",
      "        275       0.00      0.00      0.00         0\n",
      "        276       0.00      0.00      0.00         1\n",
      "        277       0.00      0.00      0.00         1\n",
      "        278       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.06      0.12      0.06       758\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_val, logreg.predict(X_val), labels=np.arange(len(issue_codes)))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classification with 15 Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_dir': '/usr/local/lib/python3.5/dist-packages/textacy/data/supreme_court', 'description': 'Collection of ~8.4k decisions issued by the U.S. Supreme Court between November 1946 and June 2016.', 'name': 'supreme_court', 'site_url': 'http://caselaw.findlaw.com/court/us-supreme-court'}\n",
      "Found 8419 texts.\n",
      "Found 15 labels.\n"
     ]
    }
   ],
   "source": [
    "sc = SupremeCourt()\n",
    "print(sc.info)\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "issue_codes = list(sc.issue_area_codes.keys())\n",
    "issue_codes.sort()\n",
    "issue_codes = [str(ic) for ic in issue_codes]\n",
    "\n",
    "labels_index = dict(zip(issue_codes, np.arange(len(issue_codes))))\n",
    "\n",
    "for i,record in enumerate(sc.records(limit=-1)):\n",
    "    if record['issue'] == None: # some cases have None as an issue\n",
    "        labels.append(labels_index['-1'])\n",
    "    else:\n",
    "        labels.append(labels_index[record['issue'][:-4]])\n",
    "    texts.append(record['text'])\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "print('Found %s labels.' % len(labels_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6819, 90018)\n",
      "(6819,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(842, 90018)\n",
      "(842,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43824228028503565\n",
      "0.45382585751978893\n"
     ]
    }
   ],
   "source": [
    "score = logreg.score(X_test, y_test)\n",
    "print(score)\n",
    "score = logreg.score(X_val, y_val)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.46      0.90      0.61       183\n",
      "          2       0.46      0.43      0.44       121\n",
      "          3       0.83      0.09      0.16        56\n",
      "          4       0.00      0.00      0.00        33\n",
      "          5       0.00      0.00      0.00         9\n",
      "          6       0.00      0.00      0.00        11\n",
      "          7       0.50      0.15      0.23        33\n",
      "          8       0.44      0.70      0.54       145\n",
      "          9       0.38      0.16      0.22       102\n",
      "         10       0.00      0.00      0.00        33\n",
      "         11       0.00      0.00      0.00         5\n",
      "         12       1.00      0.04      0.08        25\n",
      "         13       0.00      0.00      0.00         1\n",
      "         14       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.44      0.45      0.38       758\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_val, logreg.predict(X_val), labels=np.arange(len(issue_codes)))\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
